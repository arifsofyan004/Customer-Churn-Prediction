# -*- coding: utf-8 -*-
"""Challenge chapter 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yUxSBkrqsbY2gBXZbutLjesE2eEJJrCM

# Business Understanding
Untuk memahami faktor-faktor apa yang memengaruhi keputusan pelanggan untuk berhenti berlangganan (churn) atau tetap berlangganan.
yang memungkinkan perusahaan untuk mengidentifikasi pola perilaku yang berkaitan dengan churn dan mengambil tindakan yang tepat untuk mempertahankan pelanggan yang ada dan mencegah churn di masa depan. Dengan demikian, tujuan dari analisis ini adalah untuk mengembangkan pemahaman yang lebih dalam tentang perilaku pelanggan dan faktor-faktor yang mempengaruhi keputusan mereka untuk tetap menjadi pelanggan atau berhenti berlangganan.

# Data Understanding
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/FGA - Binar Academy/Dataset/Data Train.csv')
df.head()

"""### Deskripsi Kolom
- **state**: Nama negara bagian tempat pelanggan berada.
- **account_length**: Jumlah hari pelanggan telah menjadi anggota.
- **area_code**: Kode area tempat nomor telepon pelanggan berada.
- **international_plan**: Menunjukkan apakah pelanggan telah berlangganan paket rencana internasional (Yes/No).
- **voice_mail_plan**: Menunjukkan apakah pelanggan telah berlangganan paket voicemail (Yes/No).
- **number_vmail_messages**: Jumlah pesan voicemail yang diterima oleh pelanggan.
- **total_day_minutes**: Total menit yang digunakan pelanggan selama periode siang hari.
- **total_day_calls**: Total panggilan yang dilakukan pelanggan selama periode siang hari.
- **total_day_charge**: Total biaya panggilan pelanggan selama periode siang hari.
- **total_eve_minutes**: Total menit yang digunakan pelanggan selama periode malam hari.
- **total_eve_calls**: Total panggilan yang dilakukan pelanggan selama periode malam hari.
- **total_eve_charge**: Total biaya panggilan pelanggan selama periode malam hari.
- **total_night_minutes**: Total menit yang digunakan pelanggan selama periode malam hari.
- **total_night_calls**: Total panggilan yang dilakukan pelanggan selama periode malam hari.
- **total_night_charge**: Total biaya panggilan pelanggan selama periode malam hari.
- **total_intl_minutes**: Total menit panggilan internasional yang digunakan pelanggan.
- **total_intl_calls**: Total panggilan internasional yang dilakukan pelanggan.
- **total_intl_charge**: Total biaya panggilan internasional yang dikenakan kepada pelanggan.
- **number_customer_service_calls**: Jumlah panggilan layanan pelanggan yang dilakukan oleh pelanggan.
- **churn**: Menunjukkan apakah pelanggan churn atau tidak (Yes/No).

"""

df.info()

df.describe()

df.isna().sum()

df.duplicated().sum()

df.nunique()

import seaborn as sns
import matplotlib.pyplot as plt

# Menghitung matriks korelasi antar fitur
feature_correlation_matrix = df.corr(numeric_only=True)

# Plot heatmap korelasi antar fitur
plt.figure(figsize=(12, 8))
sns.heatmap(feature_correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

"""Fitur-fitur yang memiliki korelasi yang sangat tinggi dengan fitur lainnya :

- 'total_day_minutes' dan 'total_day_charge'
- 'total_eve_minutes' dan 'total_eve_charge'
- 'total_night_minutes' dan 'total_night_charge'
- 'total_intl_minutes' dan 'total_intl_charge'

Fitur biaya panggilan dihasilkan dari informasi jumlah menit yang digunakan dalam setiap periode waktu, sehingga fitur biaya panggilan dianggap redundan.

Oleh karena itu, kami memilih untuk **menghapus** fitur :
- **'total_day_charge'**
- **'total_eve_charge'**
- **'total_night_charge'**
- **total_intl_charge**

agar:
- Mengurangi dimensi dataset,
- Menghilangkan informasi yang tidak diperlukan, dan
- Mempertahankan informasi asli yang diwakili oleh jumlah menit yang digunakan.
"""

df.drop(['total_day_charge', 'total_eve_charge', 'total_night_charge', 'total_intl_charge'], axis=1, inplace=True)
df.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Menghitung matriks korelasi antar fitur
feature_correlation_matrix = df.corr(numeric_only=True)

# Plot heatmap korelasi antar fitur
plt.figure(figsize=(12, 8))
sns.heatmap(feature_correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
df['churn'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribusi Churn')
plt.xlabel('Churn')
plt.ylabel('Jumlah')
plt.xticks(rotation=0)
plt.xticks(ticks=[0, 1], labels=['Tidak Churn', 'Churn'])
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")
plt.figure(figsize=(12, 8))

for i, column in enumerate(df.select_dtypes(include=['int64', 'float64']).columns):
    plt.subplot(4, 4, i+1)
    sns.boxplot(x=df[column], color='skyblue')
    plt.title(column)

plt.tight_layout()
plt.show()

for column in df.select_dtypes(include=['int64', 'float64']).columns:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Outliers di kolom '{column}': {outliers.shape[0]}")

"""Disini saya mempertahankan outlier karena masih dianggap normal dan bisa saja benar terjadi dan dapat membantu mencerminkan variasi alami dalam dataset.

### Check Categorical Feature
"""

for col in df.select_dtypes(include='object').columns.tolist():
    print(df[col].value_counts(normalize=True)*100)
    print('\n')

"""Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini yaitu kolom **'international_plan'**"""

df.drop('international_plan', axis=1, inplace=True)
df.head()

"""# Data Preparation"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
import numpy as np

label_encoder = LabelEncoder()
df['churn'] = label_encoder.fit_transform(df['churn'])

X = df.drop(columns=['churn'])
y = df['churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

""" - Membagi data dengan perbandingan 80% untuk data pelatihan dan 20% untuk data pengujian.
 - Parameter 'stratify=y' untuk memastikan distribusi kelas target yang seimbang di kedua set data pelatihan dan pengujian.
"""

numerical_cols = ['account_length', 'number_vmail_messages', 'total_day_minutes', 'total_day_calls',
                  'total_eve_minutes', 'total_eve_calls', 'total_night_minutes', 'total_night_calls',
                  'total_intl_minutes', 'total_intl_calls', 'number_customer_service_calls']

categorical_cols = ['state', 'area_code', 'voice_mail_plan']

# Preprocessing untuk data numerik
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Preprocessing untuk data kategorikal
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Gabungkan preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

"""# Modeling

### Model Logistic Regression
"""

# Pipeline untuk model regresi logistik
logistic_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(solver='liblinear', max_iter=1000, penalty='l2', class_weight='balanced'))
])

# Menambahkan parameter yang akan diuji
param_space_lr = {
    "classifier__C": np.logspace(-3, 3, 7),
    "classifier__fit_intercept": [True, False],
}

# Melatih model dengan teknik RandomizedSearchCV
model_lr = GridSearchCV(logistic_model, param_grid=param_space_lr, cv=3)

# Fitting model ke data pelatihan
model_lr.fit(X_train, y_train)

# Tampilkan hasil regresi logistik
print("Best Parameters (Logistic Regression):", model_lr.best_params_)
print("Training Accuracy (Logistic Regression):", model_lr.score(X_train, y_train))
print("Model Best Score (Logistic Regression):", model_lr.best_score_)
print("Test Accuracy (Logistic Regression):", model_lr.score(X_test, y_test))

"""### Model XGBoost"""

# Pipeline untuk model XGBoost
xgb_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', XGBClassifier())
])

# Menambahkan parameter yang akan diuji
param_space_xgb = {
    'classifier__n_estimators': [100, 500, 1000],
    'classifier__max_depth': [3, 5, 7],
    'classifier__gamma': [0, 0.1, 0.2]
}

# Melatih model dengan teknik GridSearchCV
model_xgb = GridSearchCV(xgb_model, param_grid=param_space_xgb, cv=3)

# Fitting model ke data pelatihan
model_xgb.fit(X_train, y_train)

# Tampilkan hasil XGBoost
print("Best Parameters (XGBoost):", model_xgb.best_params_)
print("Training Accuracy (XGBoost):", model_xgb.score(X_train, y_train))
print("Model Best Score (XGBoost):", model_xgb.best_score_)
print("Test Accuracy (XGBoost):", model_xgb.score(X_test, y_test))

"""# Evaluation

### Evaluation model Logistic Regresion
"""

from sklearn.metrics import classification_report, confusion_matrix

# Prediksi label untuk data uji
y_pred_lr = model_lr.predict(X_test)

# Confusion matrix
print("\nConfusion Matrix Logistic Regression:")
print(confusion_matrix(y_test, y_pred_lr))

# Evaluasi performa model
print("\nClassification Report Logistic Regression:")
print(classification_report(y_test, y_pred_lr))

"""### Evaluation model XGBoost"""

# Memprediksi dengan model XGBoost
y_pred_xgb = model_xgb.predict(X_test)

# Confusion matrix
print("\nConfusion Matrix XGBoost:")
print(confusion_matrix(y_test, y_pred_xgb))

# Evaluasi performa model
print("\nClassification Report XGBoost:")
print(classification_report(y_test, y_pred_xgb))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Menghitung probabilitas prediksi untuk kedua model
y_proba_lr = model_lr.predict_proba(X_test)[:, 1]
y_proba_xgb = model_xgb.predict_proba(X_test)[:, 1]

# Menghitung nilai AUC untuk kedua model
auc_lr = roc_auc_score(y_test, y_proba_lr)
auc_xgb = roc_auc_score(y_test, y_proba_xgb)

# Menghitung kurva ROC
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_proba_xgb)

# Plot kurva ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.2f})', color='blue')
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.2f})', color='red')
plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

"""# Deployment"""

df_test = pd.read_csv("/content/drive/MyDrive/FGA - Binar Academy/Dataset/Data Test.csv")
df_test.head()

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

# Memprediksi churn menggunakan model regresi logistik
y_pred_lr = model_lr.predict(df_test)

# Memprediksi churn menggunakan model XGBoost
y_pred_xgb = model_xgb.predict(df_test)

# Menambahkan hasil prediksi sebagai kolom baru pada DataFrame df_test
df_test['Churn Prediction (Logistic Regression)'] = y_pred_lr
df_test['Churn Prediction (XGBoost)'] = y_pred_xgb

# Menampilkan lima baris pertama DataFrame df_test yang telah diperbarui
df_test.head()

# Save hasil prediksi
df_test.to_csv('/content/drive/MyDrive/FGA - Binar Academy/Dataset/predictions.csv', index=False)

